{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1a742b8b4f4322e668f14f041f34b4a",
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "3. You are provided with **some basic sanity checks**. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in the provided `hw1.py` python module only. All the logic you write is imported and used using this jupyter notebook.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports detected in `hw1.py` will earn you the grade of 0, even if you only used them for testing.\n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install (1.15.4). Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes the `hw1.py` file and this notebook. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Load a dataset and perform basic data exploration using a powerful data science library called [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "2. Preprocess the data for linear regression.\n",
    "3. Compute the cost and perform gradient descent in pure numpy in vectorized form.\n",
    "4. Fit a linear regression model using a single feature.\n",
    "5. Visualize your results using matplotlib.\n",
    "6. Perform multivariate linear regression.\n",
    "7. Pick the best three features in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "be1c7ede5779f9631027d0e30d850ef9",
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hw1'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-81eeb7408445>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# from .hw1 import generate_triplets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# from .hw1 import find_best_triplet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhw1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# make matplotlib figures appear inline in the notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hw1'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "from hw1 import compute_cost\n",
    "from hw1 import gradient_descent\n",
    "from hw1 import pinv\n",
    "from hw1 import efficient_gradient_descent\n",
    "from hw1 import find_best_alpha\n",
    "from hw1 import generate_triplets\n",
    "from hw1 import find_best_triplet\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1c08fc77cc2a4a6f05b2b4d974a0068",
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Data Preprocessing (10 Points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County, USA. The dataset contains 21,613 obervations with 19 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "82d5ccadf9f03b9939e0b7a34a34c74a",
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/kc_house_data.csv' does not exist",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0f36763e17ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read comma separated data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/kc_house_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Relative paths are better than absolute paths. Handle with care.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# df stands for dataframe, which is the default format for datasets in pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data/kc_house_data.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data/kc_house_data.csv') # Relative paths are better than absolute paths. Handle with care.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4548a35caa1b5e3b46e40dafbc1e468b",
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in any data-oriented project is to first try and understand the data. Fortunately, pandas is built for that purpose. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34e9b84588d56deb1426ff206c47603a",
     "grade": true,
     "grade_id": "cell-5b9d27f913f08c27",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-563ca00066ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Print the first 10 entries of the dataframe. \n",
    "\n",
    "# YOUR CODE HERE\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "67e48b4269448a1e417fedbf3a51ee18",
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id         price      bedrooms     bathrooms   sqft_living  \\\n",
      "count  2.161300e+04  2.161300e+04  21613.000000  21613.000000  21613.000000   \n",
      "mean   4.580302e+09  5.400881e+05      3.370842      2.114757   2079.899736   \n",
      "std    2.876566e+09  3.671272e+05      0.930062      0.770163    918.440897   \n",
      "min    1.000102e+06  7.500000e+04      0.000000      0.000000    290.000000   \n",
      "25%    2.123049e+09  3.219500e+05      3.000000      1.750000   1427.000000   \n",
      "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
      "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
      "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
      "\n",
      "           sqft_lot        floors    waterfront          view     condition  \\\n",
      "count  2.161300e+04  21613.000000  21613.000000  21613.000000  21613.000000   \n",
      "mean   1.510697e+04      1.494309      0.007542      0.234303      3.409430   \n",
      "std    4.142051e+04      0.539989      0.086517      0.766318      0.650743   \n",
      "min    5.200000e+02      1.000000      0.000000      0.000000      1.000000   \n",
      "25%    5.040000e+03      1.000000      0.000000      0.000000      3.000000   \n",
      "50%    7.618000e+03      1.500000      0.000000      0.000000      3.000000   \n",
      "75%    1.068800e+04      2.000000      0.000000      0.000000      4.000000   \n",
      "max    1.651359e+06      3.500000      1.000000      4.000000      5.000000   \n",
      "\n",
      "              grade    sqft_above  sqft_basement      yr_built  yr_renovated  \\\n",
      "count  21613.000000  21613.000000   21613.000000  21613.000000  21613.000000   \n",
      "mean       7.656873   1788.390691     291.509045   1971.005136     84.402258   \n",
      "std        1.175459    828.090978     442.575043     29.373411    401.679240   \n",
      "min        1.000000    290.000000       0.000000   1900.000000      0.000000   \n",
      "25%        7.000000   1190.000000       0.000000   1951.000000      0.000000   \n",
      "50%        7.000000   1560.000000       0.000000   1975.000000      0.000000   \n",
      "75%        8.000000   2210.000000     560.000000   1997.000000      0.000000   \n",
      "max       13.000000   9410.000000    4820.000000   2015.000000   2015.000000   \n",
      "\n",
      "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
      "count  21613.000000  21613.000000  21613.000000   21613.000000   21613.000000  \n",
      "mean   98077.939805     47.560053   -122.213896    1986.552492   12768.455652  \n",
      "std       53.505026      0.138564      0.140828     685.391304   27304.179631  \n",
      "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
      "25%    98033.000000     47.471000   -122.328000    1490.000000    5100.000000  \n",
      "50%    98065.000000     47.571800   -122.230000    1840.000000    7620.000000  \n",
      "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
      "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  \n"
     ]
    }
   ],
   "source": [
    "# Show the statistics of the dataset. \n",
    "\n",
    "# YOUR CODE HERE\n",
    "print (df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90c5e5a248d2315e3432cb7ef622b07e",
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Since we are dealing with simple linear regression, we will extract the target values and the `sqft_living` variable from the dataset. Use pandas and select both columns as separate variables and transform them into a numpy array. (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f9922985c8cf296ceafe252be1bc67c",
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1180 2570  770 ... 1020 1600 1020]\n"
     ]
    }
   ],
   "source": [
    "X = None # placeholder for the variables\n",
    "y = None # placeholder for the target values\n",
    "\n",
    "# YOUR CODE HERE\n",
    "y = np.array(df.get('price'))\n",
    "x = np.array(df.get('sqft_living'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "39c5e1a80080a15638c3950e894ce932",
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use mean normalization for the fearures (`X`) and scaling for the true labels (`y`).\n",
    "\n",
    "Implement the cost function `preprocess` in the python file `hw1.py`. (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8b8f69bab7b8916b112d06540d2af851",
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6eaa6ecca97ed8fea0a540124569699c",
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n",
    "Another useful tool is data visualization. Since this problem has only two parameters, it is possible to create a two-dimensional scatter plot to visualize the data. Note that many real-world datasets are highly dimensional and cannot be visualized naively. We will be using `matplotlib` for all data visualization purposes since it offers a wide range of visualization tools and is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da4a6f58757042eae1fd19a0dc2e0dbf",
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84c9d066b5b72647ef12ff314bfeea25",
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that `X` takes into consideration the bias $\\theta_0$ in the linear model. Hint, recall that the predications of our linear model are of the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "Add columns of ones as the zeroth column of `X`. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97f03ccd7ea89c742d78c915688272fa",
     "grade": true,
     "grade_id": "cell-44853962dc1651df",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fd6ebb57c0b09066a289ef19194d993",
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Single Variable Linear Regression (40 Points)\n",
    "Simple linear regression is a linear regression model with a single explanatory varaible and a single target value. \n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "Our task is to find the best possible linear line that explains all the points in our dataset. We start by guessing initial values for the linear regression parameters $\\theta$ and updating the values using gradient descent. \n",
    "\n",
    "The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{n}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where the hypothesis (model) $h_\\theta(x)$ is given by a **linear** model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "$\\theta_j$ are parameters of your model. and by changing those values accordingly you will be able to lower the cost function $J(\\theta)$. One way to accopmlish this is to use gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "In linear regresion, we know that with each step of gradient descent, the parameters $\\theta_j$ get closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7ddfe3fe53f80e2ac2cf975f1a1864f",
     "grade": false,
     "grade_id": "cell-0f83af93c0436542",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Implement the cost function `compute_cost` in the python file `hw1.py`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "82f63736b078944d8ac4b3e367c931cd",
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta = np.array([-1, 2])\n",
    "J = compute_cost(X, y, theta)\n",
    "# This is a basic test of your implementation.\n",
    "print('✓' if np.isclose(J, 0.5060823381358492, rtol=1e-08) else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95c83bf7339cfc79fa589a8ab76e75d6",
     "grade": false,
     "grade_id": "cell-afdc527b73d275bb",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Implement the gradient descent function `gradient_descent` in the python file `hw1.py` and run the following cell. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7cedf2f8fa16baec068a7037b1896b88",
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X ,y, theta, alpha, iterations)\n",
    "# This is a basic test of your implementation.\n",
    "print('✓' if np.isclose(theta, [0, 0.487640956], rtol=1e-08).all() else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ae3d99c1670088037ff28329fe63ed9f",
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help you find problems with your code. Notice that since the network converges quickly, we are using logarithmic scale for the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ea903d4560c0e9c6624d0b74ec69ae6",
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc4068497c757d630735435ea5d90135",
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Implement the psuedo-inverse function `pinv` in the python file `hw1.py`. **Do not use `np.pinv`**, instead use only direct matrix multicplication as you saw in class. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b40082f8a19c7b93c79098e70af4905e",
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X,y)\n",
    "J_pinv = compute_cost(X, y, theta_pinv)\n",
    "\n",
    "# This is a basic test of your implementation.\n",
    "print('✓' if np.isclose(J_pinv, 0.0005878100453257926, rtol=1e-08) else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss value for the theta calculated using the psuedo-inverse to our graph. This is another sanity check as the loss of our model should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40905b5c4b304f3853fa12393645070e",
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44bbdd23698fad11a848629228120177",
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "We can use a better approach for the implementation of `gradient_descent`. Instead of performing 40,000 iterations, we wish to stop when the improvement of the loss value is smaller than `1e-8` from one iteration to the next. Implement the function `efficient_gradient_descent` in the python file `hw1.py`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3de14eaeb42f690c22b07ed893914214",
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model im terms of speed and accuracy. Complete the function `find_best_alpha` in the python file `hw1.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f22763f9c81213d07fab79708686bbf",
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_dict = find_best_alpha(X, y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a3d8c316551c01759d703bb988e3355e",
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`. This can be done in several lines using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aaa819e730a9eefb441418093c65aa68",
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed5bf48af6d4beea01c0105abad9a433",
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Pick the best three alpha values you just calculated and provide **one** graph with three lines indicating the loss as a function of iterations (Use 10,000 iterations). Note you are required to provide general code for this purpose (no hard-coding). Make sure the visualziation is clear and informative. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99d51bc5a3a38f55888e31dd4ec7b2b5",
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcdaa23ffac969f68f0b204ba46d761d",
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "This is yet another sanity check. This function plots the regression lines of your model and the model based on the pseudoinverse calculation. Both models should exhibit the same trend through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f8941381b9e491baf9db93e2977988d",
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X[:,1], y, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X[:, 1], np.dot(X, theta), 'o')\n",
    "plt.plot(X[:, 1], np.dot(X, theta_pinv), '-')\n",
    "\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31e2db8ae2aae1689449bc58750d916d",
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Multivariate Linear Regression (30 points)\n",
    "\n",
    "In most cases, you will deal with databases that have more than one feature. It can be as little as two features and up to thousands of features. In those cases, we use a multiple linear regression model. The regression equation is almost the same as the simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\theta^T \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "If you wrote vectorized code, this part should be straightforward. If your code is not vectorized, you should go back and edit your functions such that they support both multivariate and single variable regression. **Your code should not check the dimensionality of the input before running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3521b4f7c72c9740e93ac0e0da5e935",
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8469c37124b6042f50feed82ade5580",
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant. The features your should ignore are the `id` and `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "543a9d438725d2f1c79582dedf8ed77a",
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40f56bb832da75e784e5f3e934e77b5a",
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Use the **same** `preprocess` function you implemented in `hw1.py`. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous part still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1121d860d9c682bb66565e8b08518876",
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3D visualization, we can still observe trends in the data. Visualizing additional dimensions requires advanced techniques we will learn later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0b219718a2cb94f9033ce52a3068081",
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = p3.Axes3D(fig)\n",
    "xx = X[:, 1][:1000]\n",
    "yy = X[:, 2][:1000]\n",
    "zz = y[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cc41adc5504de124dbd3c4079ada9a4",
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Use the bias trick again (add a column of ones as the zeroth column in `X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7beb16fb0179f9aa812160895dbf131",
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3059f9f7ed47d783e8dca31745017c56",
     "grade": false,
     "grade_id": "cell-2b89288ff61c80ac",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Make sure the functions `compute_cost` (10 points), `gradient_descent` (15 points), and `pinv` (5 points) work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the single variable regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7403184b80e207dc32c158318a7192d3",
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = None\n",
    "theta = np.ones(shape)\n",
    "J = compute_cost(X, y, theta)\n",
    "\n",
    "# This is a basic test of your implementation.\n",
    "print('✓' if np.isclose(J, 0.8527637409800696, rtol=1e-08) else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e332a1db92be37e5a116e94f4b4b115",
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = X.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "alpha = best_alpha\n",
    "theta, J_history = gradient_descent(X ,y, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7ae8e633828caec8e98f4a972dae992",
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X,y)\n",
    "J_pinv = compute_cost(X, y, theta_pinv)\n",
    "\n",
    "# This is a basic test of your implementation.\n",
    "print('✓' if np.isclose(J_pinv, 0.0004473773610022496, rtol=1e-08) else '✗')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use visualization to make sure the code works well. Notice we use logarithmic scale for the number of iterations, since gradient descent converges after 500 iterations in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "395872392a58e83d70e03701f2b2dbed",
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d42f3f4ccf725bbe915051ae56fd93c0",
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Part 3: Find best features for regression (20 points)\n",
    "\n",
    "Adding additional features to our regression model makes it more complicated but does not necessarily improves performance. Find the combination of three features that best minimizes the loss. First, we will reload the dataset as a dataframe in order to access the feature names. Use the dataframe with the relevant features as the input to the `generate_triplets` and obtain a list of all possible feature triplets. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2778323069c0e8f5e0fd4c1774b4d864",
     "grade": false,
     "grade_id": "cell-0800ea03f680996e",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'id', 'date']\n",
    "all_features = df.drop(columns=columns_to_drop)\n",
    "triplets = generate_triplets(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c70351361caa9fca3922b08074f803e8",
     "grade": false,
     "grade_id": "cell-5bf66bfd62450001",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Complete the function `find_best_triplet` in the python file `hw1.py`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ad375437e5a3b8d763121a5d3e55f81",
     "grade": false,
     "grade_id": "cell-64bf74a89ba3a0b2",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "print(find_best_triplet(df, triplets, alpha=best_alpha, num_iter=20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8e106ddb21729684001035eef9cf0b2",
     "grade": false,
     "grade_id": "cell-01736fab76114323",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Give an explanations to the results. Do they make sense? How could you further improve this linear regression model? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
